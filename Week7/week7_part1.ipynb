{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd630069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Title: Assignment 7.2\n",
    "# Author: Surenther Selvaraj\n",
    "# Date: 23 Oct 2025\n",
    "# Modified By: Surenther Selvaraj\n",
    "# Description: Dimensionality Reduction and Feature Selection\n",
    "# Data: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22355512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importing Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded successfully! ---\n",
      "\n",
      "--- First 5 Rows of the Dataset ---\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "\n",
      "Dataset shape: 1460 rows and 81 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Import the housing data as a data frame and ensure that the data is loaded properly. --- #\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'train.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"--- Data loaded successfully! ---\")\n",
    "\n",
    "    # --- Ensure the data is loaded properly ---\n",
    "    # 1. Print the first 5 rows to inspect the data\n",
    "    print(\"\\n--- First 5 Rows of the Dataset ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 2. Print the shape (rows, columns)\n",
    "    print(f\"\\nDataset shape: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- ERROR: File not found at '{file_path}' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Id' column dropped. ---\n",
      "\n",
      "--- Columns with more than 40.0% missing values dropped. ---\n",
      "Dropped columns: ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "\n",
      "New dataset shape: 1460 rows and 74 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Drop the \"Id\" column and any features that are missing more than 40% of their values. --- #\n",
    "\n",
    "# --- Drop the 'Id' column ---\n",
    "df.drop('Id', axis=1, inplace=True)\n",
    "print(\"--- 'Id' column dropped. ---\")\n",
    "\n",
    "# --- Drop columns with more than 40% missing values ---\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 40.0\n",
    "\n",
    "# Identify columns to drop\n",
    "columns_to_drop = missing_percentage[missing_percentage > threshold].index\n",
    "\n",
    "# Drop the identified columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"\\n--- Columns with more than {threshold}% missing values dropped. ---\")\n",
    "if len(columns_to_drop) > 0:\n",
    "    print(\"Dropped columns:\", list(columns_to_drop))\n",
    "else:\n",
    "    print(\"No columns met the threshold for dropping.\")\n",
    "\n",
    "# --- Verify the changes ---\n",
    "print(f\"\\nNew dataset shape: {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identified 37 numerical columns for imputation. ---\n",
      "--- Missing values in numerical columns filled with median. ---\n"
     ]
    }
   ],
   "source": [
    "# --- For numerical columns, fill in any missing data with the median value. --- #\n",
    "\n",
    "# --- Select numerical columns ---\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"--- Identified {len(numerical_cols)} numerical columns for imputation. ---\")\n",
    "\n",
    "# --- Fill missing values with the median for each column ---\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "\n",
    "print(\"--- Missing values in numerical columns filled with median. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identified 37 categorical columns for imputation. ---\n",
      "--- Missing values in categorical columns filled with mode. ---\n"
     ]
    }
   ],
   "source": [
    "# --- For categorical columns, fill in any missing data with the most common value (mode). --- #\n",
    "\n",
    "# --- Select categorical columns ---\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"--- Identified {len(categorical_cols)} categorical columns for imputation. ---\")\n",
    "\n",
    "# --- Fill missing values with the mode for each column ---\n",
    "for col in categorical_cols:\n",
    "    # .mode() returns a Series; [0] selects the first one (in case of ties).\n",
    "    most_common_value = df[col].mode()[0]\n",
    "    # Fill NaNs with this mode using inplace=True\n",
    "    df[col].fillna(most_common_value, inplace=True)\n",
    "\n",
    "print(\"--- Missing values in categorical columns filled with mode. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (before dummies): 1460 rows, 74 columns\n",
      "New shape (after dummies): 1460 rows, 230 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Convert categorical columns to dummy variables --- #\n",
    "\n",
    "# --- Store the original shape for comparison ---\n",
    "original_shape = df.shape\n",
    "print(f\"Original shape (before dummies): {original_shape[0]} rows, {original_shape[1]} columns\")\n",
    "\n",
    "# --- Convert categorical columns to dummy variables ---\n",
    "# pd.get_dummies() will automatically find and convert all 'object' type columns.\n",
    "# drop_first=True creates k-1 dummies for k categories, avoiding the dummy variable trap.\n",
    "# dtype=int ensures the new columns are 0s and 1s.\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "# --- Verify the changes ---\n",
    "new_shape = df.shape\n",
    "print(f\"New shape (after dummies): {new_shape[0]} rows, {new_shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4739c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features (X) and Target (y) successfully separated. ---\n",
      "--- Data successfully split into training and test sets. ---\n",
      "X_train shape: (1168, 229)\n",
      "X_test shape: (292, 229)\n",
      "y_train shape: (1168,)\n",
      "y_test shape: (292,)\n"
     ]
    }
   ],
   "source": [
    "# --- Split the data into a training and test set, where the SalePrice column is the target. --- #\n",
    "\n",
    "# --- Define Features (X) and Target (y) ---\n",
    "# X contains all columns *except* 'SalePrice'\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "\n",
    "# y contains *only* the 'SalePrice' column\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(\"--- Features (X) and Target (y) successfully separated. ---\")\n",
    "\n",
    "# --- Split the data ---\n",
    "# test_size=0.2 means 20% of the data will be used for testing.\n",
    "# random_state=42 is a standard number to ensure the split is reproducible.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"--- Data successfully split into training and test sets. ---\")\n",
    "\n",
    "# --- Verify the shapes of the new sets ---\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc9c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model trained successfully. ---\n",
      "\n",
      "--- Model Performance on Test Set ---\n",
      "R-squared (R2): 0.6478\n",
      "Root Mean Squared Error (RMSE): $51,973.14\n"
     ]
    }
   ],
   "source": [
    "# --- Run a linear regression and report the R2-value and RMSE on the test set. --- #\n",
    "\n",
    "# --- Create and Train the Model ---\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train (fit) the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Model trained successfully. ---\")\n",
    "\n",
    "# --- Make Predictions on the Test Set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Calculate and Report Metrics ---\n",
    "# Calculate R-squared (R2)\n",
    "# R2 measures the proportion of the variance in the target\n",
    "# that is predictable from the features.\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "# RMSE measures the average magnitude of the errors in the target's units (dollars).\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdf190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training and test features standardized. ---\n",
      "--- PCA fitted on training data. ---\n",
      "\n",
      "--- PCA Transformation Complete ---\n",
      "Original number of features: 229\n",
      "Components retained to explain 90% variance: 127\n",
      "New training data shape: (1168, 127)\n",
      "New test data shape: (292, 127)\n"
     ]
    }
   ],
   "source": [
    "# --- Fit and transform the training features with a PCA so that 90% of the variance is retained. --- #\n",
    "\n",
    "# --- Standardize the Features ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler *only* on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the *same* scaler (fit on train) to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"--- Training and test features standardized. ---\")\n",
    "\n",
    "\n",
    "# --- Initialize and Fit PCA ---\n",
    "# Set n_components=0.90 to automatically select the number of\n",
    "# components that explain 90% of the variance.\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "\n",
    "# Fit and transform the *scaled training data*\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "print(\"--- PCA fitted on training data. ---\")\n",
    "\n",
    "\n",
    "# --- Transform the Test Data ---\n",
    "# Apply the same PCA transformation (fit on train) to the test data\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# --- Report the Results ---\n",
    "original_features = X_train_scaled.shape[1]\n",
    "retained_components = pca.n_components_\n",
    "\n",
    "print(f\"\\n--- PCA Transformation Complete ---\")\n",
    "print(f\"Original number of features: {original_features}\")\n",
    "print(f\"Components retained to explain 90% variance: {retained_components}\")\n",
    "print(f\"New training data shape: {X_train_pca.shape}\")\n",
    "print(f\"New test data shape: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7ceb9",
   "metadata": {},
   "source": [
    "How many features are in the PCA-transformed matrix?\n",
    "\n",
    "Based on the above output, there are 127 features in the PCA-transformed matrix.\n",
    "\n",
    "The output line \"Components retained to explain 90% variance: 127\" shows this directly.\n",
    "\n",
    "This means PCA successfully reduced the dimensionality from 229 original features down to 127 principal components while retaining 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test features successfully transformed using the fitted PCA. ---\n",
      "New test data shape: (292, 127)\n",
      "Training data shape (for comparison): (1168, 127)\n"
     ]
    }
   ],
   "source": [
    "# --- Transform but DO NOT fit the test features with the same PCA. --- #\n",
    "\n",
    "# --- Transform the Test Data ---\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"--- Test features successfully transformed using the fitted PCA. ---\")\n",
    "\n",
    "# --- Verify the shape ---\n",
    "# The number of columns should match the components from the training (e.g., 127)\n",
    "print(f\"New test data shape: {X_test_pca.shape}\")\n",
    "print(f\"Training data shape (for comparison): {X_train_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49197f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model trained successfully on PCA data. ---\n",
      "\n",
      "--- PCA Model Performance on Test Set ---\n",
      "R-squared (R2): 0.8409\n",
      "Root Mean Squared Error (RMSE): $34,938.50\n"
     ]
    }
   ],
   "source": [
    "# --- Repeat step 7 with your PCA transformed data. --- #\n",
    "\n",
    "# --- Create and Train the Model on PCA Data ---\n",
    "# Initialize a new Linear Regression model\n",
    "model_pca = LinearRegression()\n",
    "\n",
    "# Train (fit) the model on the PCA-transformed training data\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"--- Model trained successfully on PCA data. ---\")\n",
    "\n",
    "# --- Make Predictions on the PCA Test Set ---\n",
    "# Use the trained PCA model to predict on the transformed test set\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "\n",
    "# --- Calculate and Report Metrics ---\n",
    "# Calculate R-squared (R2)\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "\n",
    "print(\"\\n--- PCA Model Performance on Test Set ---\")\n",
    "print(f\"R-squared (R2): {r2_pca:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse_pca:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training features successfully scaled with MinMaxScaler. ---\n",
      "--- Test features successfully transformed with the same scaler. ---\n",
      "\n",
      "--- Verification ---\n",
      "Original X_train min: 0.00, max: 215245.00\n",
      "Scaled X_train_minmax min: 0.00, max: 1.00\n"
     ]
    }
   ],
   "source": [
    "# --- Take your original training features (from step 6) and apply a min-max scaler to them. --- #\n",
    "\n",
    "# --- Initialize the Scaler ---\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# --- Fit and Transform the Training Data ---\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_minmax_df = pd.DataFrame(X_train_minmax, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "print(\"--- Training features successfully scaled with MinMaxScaler. ---\")\n",
    "\n",
    "# --- Transform the Test Data ---\n",
    "# Use the *same* scaler (fit on train) to transform X_test.\n",
    "# We only call .transform() here.\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_test_minmax_df = pd.DataFrame(X_test_minmax, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"--- Test features successfully transformed with the same scaler. ---\")\n",
    "\n",
    "# --- Verify the results ---\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Original X_train min: {X_train.to_numpy().min():.2f}, max: {X_train.to_numpy().max():.2f}\")\n",
    "print(f\"Scaled X_train_minmax min: {X_train_minmax.min():.2f}, max: {X_train_minmax.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dcca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Variance Threshold (threshold=0.1) Results ---\n",
      "Original number of features: 229\n",
      "Features kept (variance > 0.1): 40\n",
      "Features dropped (variance <= 0.1): 189\n",
      "\n",
      "--- List of Features DROPPED ---\n",
      "MSSubClass\n",
      "LotFrontage\n",
      "LotArea\n",
      "OverallQual\n",
      "OverallCond\n",
      "YearBuilt\n",
      "MasVnrArea\n",
      "BsmtFinSF1\n",
      "BsmtFinSF2\n",
      "BsmtUnfSF\n",
      "TotalBsmtSF\n",
      "1stFlrSF\n",
      "2ndFlrSF\n",
      "LowQualFinSF\n",
      "GrLivArea\n",
      "BsmtFullBath\n",
      "BsmtHalfBath\n",
      "FullBath\n",
      "HalfBath\n",
      "BedroomAbvGr\n",
      "KitchenAbvGr\n",
      "TotRmsAbvGrd\n",
      "Fireplaces\n",
      "GarageYrBlt\n",
      "GarageCars\n",
      "GarageArea\n",
      "WoodDeckSF\n",
      "OpenPorchSF\n",
      "EnclosedPorch\n",
      "3SsnPorch\n",
      "ScreenPorch\n",
      "PoolArea\n",
      "MiscVal\n",
      "MoSold\n",
      "MSZoning_FV\n",
      "MSZoning_RH\n",
      "Street_Pave\n",
      "LotShape_IR2\n",
      "LotShape_IR3\n",
      "LandContour_HLS\n",
      "LandContour_Low\n",
      "LandContour_Lvl\n",
      "Utilities_NoSeWa\n",
      "LotConfig_CulDSac\n",
      "LotConfig_FR2\n",
      "LotConfig_FR3\n",
      "LandSlope_Mod\n",
      "LandSlope_Sev\n",
      "Neighborhood_Blueste\n",
      "Neighborhood_BrDale\n",
      "Neighborhood_BrkSide\n",
      "Neighborhood_ClearCr\n",
      "Neighborhood_CollgCr\n",
      "Neighborhood_Crawfor\n",
      "Neighborhood_Edwards\n",
      "Neighborhood_Gilbert\n",
      "Neighborhood_IDOTRR\n",
      "Neighborhood_MeadowV\n",
      "Neighborhood_Mitchel\n",
      "Neighborhood_NPkVill\n",
      "Neighborhood_NWAmes\n",
      "Neighborhood_NoRidge\n",
      "Neighborhood_NridgHt\n",
      "Neighborhood_OldTown\n",
      "Neighborhood_SWISU\n",
      "Neighborhood_Sawyer\n",
      "Neighborhood_SawyerW\n",
      "Neighborhood_Somerst\n",
      "Neighborhood_StoneBr\n",
      "Neighborhood_Timber\n",
      "Neighborhood_Veenker\n",
      "Condition1_Feedr\n",
      "Condition1_PosA\n",
      "Condition1_PosN\n",
      "Condition1_RRAe\n",
      "Condition1_RRAn\n",
      "Condition1_RRNe\n",
      "Condition1_RRNn\n",
      "Condition2_Feedr\n",
      "Condition2_Norm\n",
      "Condition2_PosA\n",
      "Condition2_PosN\n",
      "Condition2_RRAe\n",
      "Condition2_RRAn\n",
      "Condition2_RRNn\n",
      "BldgType_2fmCon\n",
      "BldgType_Duplex\n",
      "BldgType_Twnhs\n",
      "BldgType_TwnhsE\n",
      "HouseStyle_1.5Unf\n",
      "HouseStyle_2.5Fin\n",
      "HouseStyle_2.5Unf\n",
      "HouseStyle_SFoyer\n",
      "HouseStyle_SLvl\n",
      "RoofStyle_Gambrel\n",
      "RoofStyle_Mansard\n",
      "RoofStyle_Shed\n",
      "RoofMatl_CompShg\n",
      "RoofMatl_Membran\n",
      "RoofMatl_Metal\n",
      "RoofMatl_Roll\n",
      "RoofMatl_Tar&Grv\n",
      "RoofMatl_WdShake\n",
      "RoofMatl_WdShngl\n",
      "Exterior1st_AsphShn\n",
      "Exterior1st_BrkComm\n",
      "Exterior1st_BrkFace\n",
      "Exterior1st_CBlock\n",
      "Exterior1st_CemntBd\n",
      "Exterior1st_ImStucc\n",
      "Exterior1st_Plywood\n",
      "Exterior1st_Stone\n",
      "Exterior1st_Stucco\n",
      "Exterior1st_WdShing\n",
      "Exterior2nd_AsphShn\n",
      "Exterior2nd_Brk Cmn\n",
      "Exterior2nd_BrkFace\n",
      "Exterior2nd_CBlock\n",
      "Exterior2nd_CmentBd\n",
      "Exterior2nd_ImStucc\n",
      "Exterior2nd_Other\n",
      "Exterior2nd_Plywood\n",
      "Exterior2nd_Stone\n",
      "Exterior2nd_Stucco\n",
      "Exterior2nd_Wd Shng\n",
      "ExterQual_Fa\n",
      "ExterCond_Fa\n",
      "ExterCond_Gd\n",
      "ExterCond_Po\n",
      "Foundation_Slab\n",
      "Foundation_Stone\n",
      "Foundation_Wood\n",
      "BsmtQual_Fa\n",
      "BsmtCond_Gd\n",
      "BsmtCond_Po\n",
      "BsmtCond_TA\n",
      "BsmtExposure_Gd\n",
      "BsmtExposure_Mn\n",
      "BsmtFinType1_BLQ\n",
      "BsmtFinType1_LwQ\n",
      "BsmtFinType1_Rec\n",
      "BsmtFinType2_BLQ\n",
      "BsmtFinType2_GLQ\n",
      "BsmtFinType2_LwQ\n",
      "BsmtFinType2_Rec\n",
      "BsmtFinType2_Unf\n",
      "Heating_GasA\n",
      "Heating_GasW\n",
      "Heating_Grav\n",
      "Heating_OthW\n",
      "Heating_Wall\n",
      "HeatingQC_Fa\n",
      "HeatingQC_Po\n",
      "CentralAir_Y\n",
      "Electrical_FuseF\n",
      "Electrical_FuseP\n",
      "Electrical_Mix\n",
      "Electrical_SBrkr\n",
      "KitchenQual_Fa\n",
      "Functional_Maj2\n",
      "Functional_Min1\n",
      "Functional_Min2\n",
      "Functional_Mod\n",
      "Functional_Sev\n",
      "Functional_Typ\n",
      "GarageType_Basment\n",
      "GarageType_BuiltIn\n",
      "GarageType_CarPort\n",
      "GarageQual_Fa\n",
      "GarageQual_Gd\n",
      "GarageQual_Po\n",
      "GarageQual_TA\n",
      "GarageCond_Fa\n",
      "GarageCond_Gd\n",
      "GarageCond_Po\n",
      "GarageCond_TA\n",
      "PavedDrive_P\n",
      "PavedDrive_Y\n",
      "SaleType_CWD\n",
      "SaleType_Con\n",
      "SaleType_ConLD\n",
      "SaleType_ConLI\n",
      "SaleType_ConLw\n",
      "SaleType_New\n",
      "SaleType_Oth\n",
      "SaleCondition_AdjLand\n",
      "SaleCondition_Alloca\n",
      "SaleCondition_Family\n",
      "SaleCondition_Partial\n"
     ]
    }
   ],
   "source": [
    "# --- Find the min-max scaled features in your training set that have a variance above 0.1 --- #\n",
    "\n",
    "# --- Initialize and Fit the Selector ---\n",
    "# Create the selector to find features with variance > 0.1\n",
    "var_selector = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "# Fit the selector to the scaled training data\n",
    "var_selector.fit(X_train_minmax_df)\n",
    "\n",
    "# --- Get the Results ---\n",
    "# .get_support() returns a boolean mask (True/False) for each feature\n",
    "features_to_keep_mask = var_selector.get_support()\n",
    "\n",
    "# Use the mask to get the *names* of the features to keep\n",
    "features_kept = X_train_minmax_df.columns[features_to_keep_mask].tolist()\n",
    "\n",
    "# Use the inverted mask (~) to get the names of the features to drop\n",
    "features_dropped = X_train_minmax_df.columns[~features_to_keep_mask].tolist()\n",
    "\n",
    "# --- 3. Report the Findings ---\n",
    "print(f\"--- Variance Threshold (threshold=0.1) Results ---\")\n",
    "print(f\"Original number of features: {X_train_minmax_df.shape[1]}\")\n",
    "print(f\"Features kept (variance > 0.1): {len(features_kept)}\")\n",
    "print(f\"Features dropped (variance <= 0.1): {len(features_dropped)}\")\n",
    "\n",
    "print(\"\\n--- List of Features DROPPED ---\")\n",
    "if len(features_dropped) > 0:\n",
    "    for f in features_dropped:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"No features were dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e4444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_test shape: (292, 229)\n",
      "New X_test shape after scaling and thresholding: (292, 40)\n"
     ]
    }
   ],
   "source": [
    "# --- Transform but DO NOT fit the test features with the same steps applied in steps 11 and 12. --- #\n",
    "\n",
    "# --- Apply the fitted MinMaxScaler ---\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "# --- Convert back to DataFrame ---\n",
    "X_test_minmax_df = pd.DataFrame(X_test_minmax, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- Apply the fitted VarianceThreshold (from Part 12) ---\n",
    "X_test_vt = var_selector.transform(X_test_minmax_df)\n",
    "\n",
    "# --- Report the change in shape ---\n",
    "print(f\"Original X_test shape: {X_test.shape}\")\n",
    "print(f\"New X_test shape after scaling and thresholding: {X_test_vt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "336f70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final training data created with shape: (1168, 40) ---\n",
      "--- Model trained successfully on high-variance data. ---\n",
      "\n",
      "--- High-Variance Model Performance on Test Set ---\n",
      "R-squared (R2): 0.6481\n",
      "Root Mean Squared Error (RMSE): $51,952.01\n"
     ]
    }
   ],
   "source": [
    "# --- Repeat step 7 with the high variance data. --- #\n",
    "\n",
    "# --- Transform the Training Data ---\n",
    "X_train_vt = var_selector.transform(X_train_minmax_df)\n",
    "\n",
    "print(f\"--- Final training data created with shape: {X_train_vt.shape} ---\")\n",
    "\n",
    "\n",
    "# --- Create and Train the Model ---\n",
    "model_vt = LinearRegression()\n",
    "\n",
    "# Train (fit) the model on the high-variance training data\n",
    "model_vt.fit(X_train_vt, y_train)\n",
    "print(\"--- Model trained successfully on high-variance data. ---\")\n",
    "\n",
    "\n",
    "# --- Make Predictions on the High-Variance Test Set ---\n",
    "y_pred_vt = model_vt.predict(X_test_vt)\n",
    "\n",
    "\n",
    "# --- Calculate and Report Metrics ---\n",
    "# Calculate R-squared (R2)\n",
    "r2_vt = r2_score(y_test, y_pred_vt)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_vt = np.sqrt(mean_squared_error(y_test, y_pred_vt))\n",
    "\n",
    "print(\"\\n--- High-Variance Model Performance on Test Set ---\")\n",
    "print(f\"R-squared (R2): {r2_vt:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse_vt:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff864107",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This notebook compared three methods for predicting house prices. First, a baseline linear regression model was trained using all 229 preprocessed features, but it performed poorly, only explaining about 65% of the price variance (R²: 0.6478) with an average error of nearly $52,000 (RMSE).\n",
    "\n",
    "The second method used PCA to reduce the 229 features to 127 principal components. This model was far superior, explaining 84% of the variance (R²: 0.8409) and reducing the average error to just $35,000 (RMSE).\n",
    "\n",
    "The third method used a Variance Threshold to select the 40 \"highest variance\" features. This model performed almost identically to the baseline (R²: 0.6481), showing no improvement.\n",
    "\n",
    "From these we can understand that simply having more features (229) isn't always better. The baseline model was likely \"noisy.\" PCA created a much more accurate and efficient model by compressing the features and filtering out that noise. The Variance Threshold method, while simple, failed to select a useful set of features, proving that PCA was the clearly superior strategy for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
