{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd630069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Title: Assignment 7.2\n",
    "# Author: Surenther Selvaraj\n",
    "# Date: 23 Oct 2025\n",
    "# Modified By: Surenther Selvaraj\n",
    "# Description: Dimensionality Reduction and Feature Selection\n",
    "# Data: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22355512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importing Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef29e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data loaded successfully! ---\n",
      "\n",
      "--- First 5 Rows of the Dataset ---\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "\n",
      "Dataset shape: 1460 rows and 81 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Import the housing data ---\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'train.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"--- Data loaded successfully! ---\")\n",
    "\n",
    "    # --- Ensure the data is loaded properly ---\n",
    "    # 1. Print the first 5 rows to inspect the data\n",
    "    print(\"\\n--- First 5 Rows of the Dataset ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 2. Print the shape (rows, columns)\n",
    "    print(f\"\\nDataset shape: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- ERROR: File not found at '{file_path}' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a3c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'Id' column dropped. ---\n",
      "\n",
      "--- Columns with more than 40.0% missing values dropped. ---\n",
      "Dropped columns: ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "\n",
      "New dataset shape: 1460 rows and 74 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Drop the 'Id' column ---\n",
    "df.drop('Id', axis=1, inplace=True)\n",
    "print(\"--- 'Id' column dropped. ---\")\n",
    "\n",
    "# --- Drop columns with more than 40% missing values ---\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 40.0\n",
    "\n",
    "# Identify columns to drop\n",
    "columns_to_drop = missing_percentage[missing_percentage > threshold].index\n",
    "\n",
    "# Drop the identified columns\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print(f\"\\n--- Columns with more than {threshold}% missing values dropped. ---\")\n",
    "if len(columns_to_drop) > 0:\n",
    "    print(\"Dropped columns:\", list(columns_to_drop))\n",
    "else:\n",
    "    print(\"No columns met the threshold for dropping.\")\n",
    "\n",
    "# --- Verify the changes ---\n",
    "print(f\"\\nNew dataset shape: {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c165d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identified 37 numerical columns for imputation. ---\n",
      "--- Missing values in numerical columns filled with median. ---\n"
     ]
    }
   ],
   "source": [
    "# --- Select numerical columns ---\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"--- Identified {len(numerical_cols)} numerical columns for imputation. ---\")\n",
    "\n",
    "# --- Fill missing values with the median for each column ---\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "\n",
    "print(\"--- Missing values in numerical columns filled with median. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c470fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identified 37 categorical columns for imputation. ---\n",
      "--- Missing values in categorical columns filled with mode. ---\n"
     ]
    }
   ],
   "source": [
    "# --- Select categorical columns ---\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"--- Identified {len(categorical_cols)} categorical columns for imputation. ---\")\n",
    "\n",
    "# --- Fill missing values with the mode for each column ---\n",
    "for col in categorical_cols:\n",
    "    # .mode() returns a Series; [0] selects the first one (in case of ties).\n",
    "    most_common_value = df[col].mode()[0]\n",
    "    # Fill NaNs with this mode using inplace=True\n",
    "    df[col].fillna(most_common_value, inplace=True)\n",
    "\n",
    "print(\"--- Missing values in categorical columns filled with mode. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e65574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (before dummies): 1460 rows, 230 columns\n",
      "New shape (after dummies): 1460 rows, 230 columns\n"
     ]
    }
   ],
   "source": [
    "# --- Store the original shape for comparison ---\n",
    "original_shape = df.shape\n",
    "print(f\"Original shape (before dummies): {original_shape[0]} rows, {original_shape[1]} columns\")\n",
    "\n",
    "# --- Convert categorical columns to dummy variables ---\n",
    "# pd.get_dummies() will automatically find and convert all 'object' type columns.\n",
    "# drop_first=True creates k-1 dummies for k categories, avoiding the dummy variable trap.\n",
    "# dtype=int ensures the new columns are 0s and 1s.\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "# --- Verify the changes ---\n",
    "new_shape = df.shape\n",
    "print(f\"New shape (after dummies): {new_shape[0]} rows, {new_shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4739c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Features (X) and Target (y) successfully separated. ---\n",
      "--- Data successfully split into training and test sets. ---\n",
      "X_train shape: (1168, 229)\n",
      "X_test shape: (292, 229)\n",
      "y_train shape: (1168,)\n",
      "y_test shape: (292,)\n"
     ]
    }
   ],
   "source": [
    "# --- Define Features (X) and Target (y) ---\n",
    "# X contains all columns *except* 'SalePrice'\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "\n",
    "# y contains *only* the 'SalePrice' column\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(\"--- Features (X) and Target (y) successfully separated. ---\")\n",
    "\n",
    "# --- Split the data ---\n",
    "# test_size=0.2 means 20% of the data will be used for testing.\n",
    "# random_state=42 is a standard number to ensure the split is reproducible.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"--- Data successfully split into training and test sets. ---\")\n",
    "\n",
    "# --- Verify the shapes of the new sets ---\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ffc9c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model trained successfully. ---\n",
      "\n",
      "--- Model Performance on Test Set ---\n",
      "R-squared (R2): 0.6478\n",
      "Root Mean Squared Error (RMSE): $51,973.14\n"
     ]
    }
   ],
   "source": [
    "# --- Create and Train the Model ---\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train (fit) the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Model trained successfully. ---\")\n",
    "\n",
    "# --- Make Predictions on the Test Set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Calculate and Report Metrics ---\n",
    "# Calculate R-squared (R2)\n",
    "# R2 measures the proportion of the variance in the target\n",
    "# that is predictable from the features.\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "# RMSE measures the average magnitude of the errors in the target's units (dollars).\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ebdf190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training and test features standardized. ---\n",
      "--- PCA fitted on training data. ---\n",
      "\n",
      "--- PCA Transformation Complete ---\n",
      "Original number of features: 229\n",
      "Components retained to explain 90% variance: 127\n",
      "New training data shape: (1168, 127)\n",
      "New test data shape: (292, 127)\n"
     ]
    }
   ],
   "source": [
    "# --- Standardize the Features ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler *only* on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the *same* scaler (fit on train) to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"--- Training and test features standardized. ---\")\n",
    "\n",
    "\n",
    "# --- Initialize and Fit PCA ---\n",
    "# Set n_components=0.90 to automatically select the number of\n",
    "# components that explain 90% of the variance.\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "\n",
    "# Fit and transform the *scaled training data*\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "print(\"--- PCA fitted on training data. ---\")\n",
    "\n",
    "\n",
    "# --- Transform the Test Data ---\n",
    "# Apply the same PCA transformation (fit on train) to the test data\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# --- Report the Results ---\n",
    "original_features = X_train_scaled.shape[1]\n",
    "retained_components = pca.n_components_\n",
    "\n",
    "print(f\"\\n--- PCA Transformation Complete ---\")\n",
    "print(f\"Original number of features: {original_features}\")\n",
    "print(f\"Components retained to explain 90% variance: {retained_components}\")\n",
    "print(f\"New training data shape: {X_train_pca.shape}\")\n",
    "print(f\"New test data shape: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7ceb9",
   "metadata": {},
   "source": [
    "Based on the above output, there are 127 features in the PCA-transformed matrix.\n",
    "\n",
    "The output line \"Components retained to explain 90% variance: 127\" shows this directly.\n",
    "\n",
    "This means PCA successfully reduced the dimensionality from 229 original features down to 127 principal components while retaining 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e586901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test features successfully transformed using the fitted PCA. ---\n",
      "New test data shape: (292, 127)\n",
      "Training data shape (for comparison): (1168, 127)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Transform the Test Data ---\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"--- Test features successfully transformed using the fitted PCA. ---\")\n",
    "\n",
    "# --- 2. Verify the shape ---\n",
    "# The number of columns should match the components from the training (e.g., 127)\n",
    "print(f\"New test data shape: {X_test_pca.shape}\")\n",
    "print(f\"Training data shape (for comparison): {X_train_pca.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
