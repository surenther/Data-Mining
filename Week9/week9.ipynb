{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ab2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Title: Assignment 9.2\n",
    "# Author: Surenther Selvaraj\n",
    "# Date: 05 Nov 2025\n",
    "# Modified By: Surenther Selvaraj\n",
    "# Description: Best Model Selection and Hyperparameter Tuning\n",
    "# Data: https://www.kaggle.com/datasets/granjithkumar/loan-approval-data-set\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32c26d",
   "metadata": {},
   "source": [
    "### Import the dataset and ensure that it loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e264a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'Loan_Train.csv' imported successfully from local file.\n",
      "\n",
      "Head (First 5 Rows):\n",
      "     Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path. Ensure this path is correct if the file isn't in the same directory.\n",
    "file_name = 'Loan_Train.csv'\n",
    "\n",
    "# Check if the file exists locally before attempting to read\n",
    "if not os.path.exists(file_name):\n",
    "    print(f\"Error: The file '{file_name}' was not found in the current directory.\")\n",
    "    print(\"Please ensure the file is in the same location as your script or notebook.\")\n",
    "    df = None\n",
    "else:\n",
    "    # Attempt to import the dataset\n",
    "    try:\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f\"Dataset '{file_name}' imported successfully from local file.\")\n",
    "        \n",
    "        # Display the first 5 rows\n",
    "        print(\"\\nHead (First 5 Rows):\\n\", df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b974188",
   "metadata": {},
   "source": [
    "### Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3baefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping Loan_ID: (614, 13)\n",
      "Shape after dropping Loan_ID: (614, 12)\n",
      "\n",
      "Number of rows before dropping missing values: 614\n",
      "Number of rows after dropping missing values: 480\n",
      "\n",
      "Shape before creating dummy variables: (480, 12)\n",
      "Shape after creating dummy variables: (480, 15)\n",
      "\n",
      "--- Final Prepared DataFrame (df_model) Head ---\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "5             5417             4196.0       267.0             360.0   \n",
      "\n",
      "   Credit_History  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\n",
      "1             1.0         True         True          True         False   \n",
      "2             1.0         True         True         False         False   \n",
      "3             1.0         True         True         False         False   \n",
      "4             1.0         True        False         False         False   \n",
      "5             1.0         True         True         False          True   \n",
      "\n",
      "   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "1          False                   False              False   \n",
      "2          False                   False               True   \n",
      "3          False                    True              False   \n",
      "4          False                   False              False   \n",
      "5          False                   False               True   \n",
      "\n",
      "   Property_Area_Semiurban  Property_Area_Urban  Loan_Status_Y  \n",
      "1                    False                False          False  \n",
      "2                    False                 True           True  \n",
      "3                    False                 True           True  \n",
      "4                    False                 True           True  \n",
      "5                    False                 True           True  \n"
     ]
    }
   ],
   "source": [
    "# Drop the column \"Loan_ID\"\n",
    "# This is an identifier and is not useful for model training.\n",
    "print(\"Shape before dropping Loan_ID:\", df.shape)\n",
    "df = df.drop('Loan_ID', axis=1)\n",
    "print(\"Shape after dropping Loan_ID:\", df.shape)\n",
    "\n",
    "# Drop any rows with missing data (Imputation is skipped for this exercise)\n",
    "print(\"\\nNumber of rows before dropping missing values:\", len(df))\n",
    "df = df.dropna()\n",
    "print(\"Number of rows after dropping missing values:\", len(df))\n",
    "\n",
    "# Convert the categorical features into dummy variables (One-Hot Encoding)\n",
    "# pandas get_dummies automatically identifies and converts object/category types\n",
    "print(\"\\nShape before creating dummy variables:\", df.shape)\n",
    "df_model = pd.get_dummies(df, drop_first=True)\n",
    "print(\"Shape after creating dummy variables:\", df_model.shape)\n",
    "\n",
    "# Display the first few rows of the final prepared DataFrame\n",
    "print(\"\\n--- Final Prepared DataFrame (df_model) Head ---\")\n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e99b44",
   "metadata": {},
   "source": [
    "### Split the data into a training and test set, where the ‚ÄúLoan_Status‚Äù column is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c5956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Split Verification ---\n",
      "Total features (X) shape: (480, 14)\n",
      "Total target (y) shape: (480,)\n",
      "-----------------------------------\n",
      "X_train shape: (384, 14)\n",
      "X_test shape: (96, 14)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (96,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "target_column = 'Loan_Status_Y' \n",
    "\n",
    "# X contains all columns except the target\n",
    "X = df_model.drop(target_column, axis=1)\n",
    "\n",
    "# y contains only the target column\n",
    "y = df_model[target_column]\n",
    "\n",
    "# Split the data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y # Stratify ensures the train and test sets have the same proportion of target classes\n",
    ")\n",
    "\n",
    "# Print the resulting shapes to verify the split\n",
    "print(\"--- Data Split Verification ---\")\n",
    "print(f\"Total features (X) shape: {X.shape}\")\n",
    "print(f\"Total target (y) shape: {y.shape}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7469c",
   "metadata": {},
   "source": [
    "### Create a pipeline with a min-max scaler and a KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05679c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pipeline Structure ---\n",
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('knn', KNeighborsClassifier())])\n",
      "----------------------------\n",
      "Pipeline created successfully. It is ready for training (fitting).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the steps in the pipeline:\n",
    "# 1. 'scaler': MinMaxScaler\n",
    "# 2. 'knn': KNeighborsClassifier (using default k=5)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Print the pipeline structure to confirm the steps\n",
    "print(\"--- Pipeline Structure ---\")\n",
    "print(pipeline)\n",
    "print(\"-\" * 28)\n",
    "print(\"Pipeline created successfully. It is ready for training (fitting).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58f142",
   "metadata": {},
   "source": [
    "### Create a search space for your KNN classifier where your ‚Äún_neighbors‚Äù parameter varies from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a2f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the KNN Pipeline to the training data (X_train, y_train)...\n",
      "Pipeline fitting complete.\n",
      "\n",
      "--- Model Performance on Test Set ---\n",
      "Model Accuracy on the Test Set: 0.7083\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "print(\"Fitting the KNN Pipeline to the training data (X_train, y_train)...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting complete.\")\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "# The score method automatically calculates the accuracy for classification models.\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "# Report the accuracy\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"Model Accuracy on the Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a427e1",
   "metadata": {},
   "source": [
    "### Fit a default KNN classifier to the data with this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318ad0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the KNN Pipeline to the training data (X_train, y_train)...\n",
      "Pipeline fitting complete.\n",
      "\n",
      "--- Model Performance on Test Set ---\n",
      "Model Accuracy on the Test Set: 0.7083\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the training data\n",
    "print(\"Fitting the KNN Pipeline to the training data (X_train, y_train)...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitting complete.\")\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "# The score method automatically calculates the accuracy for classification models.\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "# Report the accuracy\n",
    "print(\"\\n--- Model Performance on Test Set ---\")\n",
    "print(f\"Model Accuracy on the Test Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a34240",
   "metadata": {},
   "source": [
    "### Find the accuracy of the grid search best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8e3257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search (testing 40 different models)...\n",
      "Grid Search complete.\n",
      "\n",
      "--- Grid Search Results ---\n",
      "Best cross-validation accuracy: 0.7604\n",
      "Best parameters found: {'knn__n_neighbors': 20, 'knn__weights': 'distance'}\n",
      "\n",
      "--- Best Model Performance on Test Set ---\n",
      "Accuracy of the Grid Search Best Model on the Test Set: 0.7188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': range(1, 21),  # Test k from 1 to 20\n",
    "    'knn__weights': ['uniform', 'distance'] # Test uniform and distance weighting\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation will be used on the training data.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "# This step performs the cross-validation and hyperparameter tuning.\n",
    "print(\"Starting Grid Search (testing 40 different models)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# Report the Best Parameters found\n",
    "print(\"\\n--- Grid Search Results ---\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "# The 'best_estimator_' attribute is the fitted pipeline with the optimal parameters.\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "# Report the accuracy of the best model on the test set\n",
    "print(\"\\n--- Best Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy of the Grid Search Best Model on the Test Set: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e793aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined Grid Search across KNN, Logistic Regression, and Random Forest...\n",
      "Grid Search complete.\n",
      "\n",
      "--- Combined Grid Search Results ---\n",
      "Best Model Found: LogisticRegression\n",
      "Best Cross-Validation Accuracy: 0.8020\n",
      "Best Parameters Found: {'classifier': LogisticRegression(random_state=42, solver='liblinear'), 'classifier__C': np.float64(0.046415888336127774), 'classifier__penalty': 'l1'}\n",
      "\n",
      "--- Best Model Performance on Test Set ---\n",
      "Accuracy of the Grid Search Best Model on the Test Set: 0.8333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# --- Define the Multi-Model Pipeline ---\n",
    "# The classifier is just a placeholder; the grid search will swap it out.\n",
    "pipeline_multi = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classifier', LogisticRegression()) \n",
    "])\n",
    "\n",
    "# --- Define the Expanded Parameter Grids ---\n",
    "\n",
    "# Grid 1: K-Nearest Neighbors (KNN)\n",
    "knn_params = {\n",
    "    'classifier': [KNeighborsClassifier()],\n",
    "    'classifier__n_neighbors': range(1, 21),\n",
    "    'classifier__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Grid 2: Logistic Regression (LogReg)\n",
    "# Based on Section 12.3: L1/L2 and C logspace\n",
    "logreg_params = {\n",
    "    'classifier': [LogisticRegression(random_state=42, solver='liblinear')],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__C': np.logspace(-4, 4, 10) # 10 values from 0.0001 to 10000\n",
    "}\n",
    "\n",
    "# Grid 3: Random Forest (RF)\n",
    "# Based on Section 12.3: n_estimators, max_features, max_depth\n",
    "forest_params = {\n",
    "    'classifier': [RandomForestClassifier(random_state=42)],\n",
    "    'classifier__n_estimators': [10, 50, 100], \n",
    "    'classifier__max_features': [1, 2, 'sqrt'], \n",
    "    'classifier__max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# Combine all parameter grids into a list\n",
    "param_grids = [knn_params, logreg_params, forest_params]\n",
    "\n",
    "# --- Initialize and Run Grid Search ---\n",
    "# We pass the list of grids to param_grid\n",
    "grid_search_multi = GridSearchCV(\n",
    "    estimator=pipeline_multi,\n",
    "    param_grid=param_grids,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "print(\"Starting combined Grid Search across KNN, Logistic Regression, and Random Forest...\")\n",
    "# Fit the grid search (assuming X_train, y_train are in memory)\n",
    "grid_search_multi.fit(X_train, y_train)\n",
    "print(\"Grid Search complete.\")\n",
    "\n",
    "# --- Report Results ---\n",
    "\n",
    "# Find the best model's class name for reporting\n",
    "best_model_name = type(grid_search_multi.best_estimator_.named_steps['classifier']).__name__\n",
    "\n",
    "print(\"\\n--- Combined Grid Search Results ---\")\n",
    "print(f\"Best Model Found: {best_model_name}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_multi.best_score_:.4f}\")\n",
    "print(f\"Best Parameters Found: {grid_search_multi.best_params_}\")\n",
    "\n",
    "# --- Report Accuracy on the Test Set ---\n",
    "# Use the best_estimator_ found by the search to score the test set\n",
    "test_accuracy = grid_search_multi.score(X_test, y_test)\n",
    "\n",
    "print(\"\\n--- Best Model Performance on Test Set ---\")\n",
    "print(f\"Accuracy of the Grid Search Best Model on the Test Set: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba15d7d",
   "metadata": {},
   "source": [
    "üí° Key Observations\n",
    "\n",
    "The primary observation is the significant impact of model selection and hyperparameter tuning on predictive performance:\n",
    "\n",
    "    Scaling and KNN: While essential, scaling alone only yielded modest performance with the default KNN model (70.83%). Tuning the KNN parameters resulted in only a minor improvement (+1.05%), indicating that the KNN algorithm is not the strongest fit for this dataset.\n",
    "\n",
    "    Superior Performance of Logistic Regression: The expanded grid search highlighted that the Logistic Regression model, even within a vast multi-model search space, was the best choice. It achieved the highest test accuracy of 83.33%, a substantial jump of over 11 percentage points from the tuned KNN model.\n",
    "\n",
    "    Optimal Hyperparameters: The best model utilized Logistic Regression with penalty='l1' and a specific C value (approximately 0.046), suggesting that a degree of regularization (specifically L1, which helps with feature selection) is crucial for achieving peak performance on this loan approval data. This confirms that selecting the right algorithm is often more important than fine-tuning a sub-optimal one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
