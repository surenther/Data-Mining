{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d00f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Title: Assignment 8.2\n",
    "# Author: Surenther Selvaraj\n",
    "# Date: 01 Nov 2025\n",
    "# Modified By: Surenther Selvaraj\n",
    "# Description: Term Project Milestone 2: Data Preparation\n",
    "# Data: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba898aa",
   "metadata": {},
   "source": [
    "## Term Project Milestone 2: Data Preparation Plan\n",
    "This document outlines the initial data preparation and feature engineering strategy for the classification model, specifically focusing on identifying and dropping features that are non-predictive, redundant, or problematic for the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a04921",
   "metadata": {},
   "source": [
    "### 1. Feature Drop Strategy\n",
    "\n",
    "To optimize the model's performance and prevent data leakage or multicollinearity, several features from the raw dataset will be excluded.\n",
    "\n",
    "| Feature to Drop  | Rationale for Exclusion  |  Category |\n",
    "|---|---|---|\n",
    "| ID  | This is a unique customer identifier. It has no predictive power and should be dropped to prevent the model from memorizing individual observations.  | Non-Predictive  |\n",
    "| Dt_Customer  | Date of customer enrollment. While conversion time might be relevant, the more actionable and direct time-based feature, Recency, is already available and highly predictive (as shown in Graph 3).  | Redundant/High Cardinality  |\n",
    "| Z_CostContact  | Standardized cost of customer contact. This column is constant across all customers in the dataset (a value of 3) and therefore provides zero variance or predictive information.  | Zero-Variance |\n",
    "| Z_Revenue | Standardized revenue from contact. This column is constant across all customers (a value of 11) and provides zero variance or predictive information.  | Zero-Variance |\n",
    "| AcceptedCmp1 to AcceptedCmp5 | Response to previous campaigns. These features represent the outcome of past offers. Using them to predict the current campaign's Response (which is effectively AcceptedCmp6) creates severe data leakage, as a positive response to a previous similar campaign is an unnaturally strong predictor for the current one. Dropping these ensures the model generalizes to new campaigns.  | Data Leakage |\n",
    "| Complain | Customer complaint status. While technically a potential predictor, the count of complaints is extremely low, leading to highly skewed classes that provide minimal signal for a classification model focused on positive response prediction.  | Near Zero-Variance |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22903bb9",
   "metadata": {},
   "source": [
    "### 2. Features to Keep and Transform\n",
    "\n",
    "The remaining features will be kept as they are highly relevant to the business problem (customer wealth, engagement, and past purchase behavior).\n",
    "\n",
    "| Feature Type  | Features to Keep |  Required Transformation |\n",
    "|---|---|---|\n",
    "| Target  | Response  | None (already binary: 0 or 1)  |\n",
    "| Demographic  | Income, Year_Birth (used to calculate Age), Marital_Status, Education, Kidhome, Teenhome  | Age is derived. Marital_Status and Education require one-hot encoding.  |\n",
    "| Behavior  | Recency | None (numerical, already normalized).  |\n",
    "| Spending  | MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds | None (numerical, may require scaling).  |\n",
    "| Channel  | NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth | None (numerical, may require scaling).  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a5fb18",
   "metadata": {},
   "source": [
    "### 1. Loading Data/Extraction\n",
    "\n",
    "This initial step is responsible for importing the necessary pandas library and defining the file path and feature lists required for the project. It explicitly defines the CHANNEL_FEATURES (e.g., NumWebPurchases) and other CORE_FEATURES (e.g., Income, Response) that are central to the customer campaign response model. The code attempts to load the marketing_campaign.csv file, and includes error handling to notify the user if the data file cannot be located. Successfully loading the data sets the stage for all subsequent cleaning and preparation activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b780d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2240 rows from marketing_campaign.csv.\n"
     ]
    }
   ],
   "source": [
    "# Python Data Extraction for Channel Features\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path (Assuming the file is in the same directory)\n",
    "FILE_PATH = 'marketing_campaign.csv'\n",
    "\n",
    "# Define the list of channel features selected from the Milestone 2 plan\n",
    "CHANNEL_FEATURES = [\n",
    "    'NumDealsPurchases',\n",
    "    'NumWebPurchases',\n",
    "    'NumCatalogPurchases',\n",
    "    'NumStorePurchases',\n",
    "    'NumWebVisitsMonth'\n",
    "]\n",
    "\n",
    "# Add other core features necessary for context (e.g., Target, Income)\n",
    "# NOTE: We temporarily include all spending features to create the useful engineered feature below.\n",
    "CORE_FEATURES = ['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Response', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH, sep='\\t')\n",
    "    print(f\"Successfully loaded {len(df)} rows from {FILE_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {FILE_PATH}. Please check the path and filename.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98221b",
   "metadata": {},
   "source": [
    "### 2. Deal with missing data\n",
    "This crucial step addresses the small number of missing values (24 rows, or 1.07%) found in the highly critical Income predictor variable. Rather than dropping these few rows, which is often acceptable but can sometimes discard valuable information, the code implements median imputation. Using the median is preferred because Income is known to be heavily skewed and contain outliers, ensuring the imputed values do not unduly distort the feature's statistical distribution. This method maintains the full dataset size while creating a complete column ready for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e5c709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing 'Income' values using the median. Total rows: 2240\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# --- Data Cleaning (Handling Missing Values) ---\n",
    "\n",
    "# Rationale: 'Income' has 24 missing values (approx. 1.07%). \n",
    "# Because Income is highly skewed (as seen in EDA), median imputation is preferred over dropping \n",
    "# rows or using the mean, as it minimizes distortion to the distribution.\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df['Income'] = imputer.fit_transform(df[['Income']])\n",
    "\n",
    "# Check the dataset size after imputation (no rows dropped)\n",
    "print(f\"Imputed missing 'Income' values using the median. Total rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0382b",
   "metadata": {},
   "source": [
    "### 3. Feature Selection\n",
    "This step focuses on creating a clean, actionable subset of the data by explicitly selecting and combining the core features identified as most relevant for the classification model. It merges the categorical, numerical, and target variables (including all spending features needed for engineering) into the df_extracted DataFrame. Within this step, a preliminary engineered feature, Total_Purchases_Count, is calculated by summing the three main purchase channel counts. This count gives an initial, unscaled metric of overall customer engagement for immediate assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606bb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Total Purchases Count Summary ---\n",
      "count    2240.000000\n",
      "mean       12.537054\n",
      "std         7.205741\n",
      "min         0.000000\n",
      "25%         6.000000\n",
      "50%        12.000000\n",
      "75%        18.000000\n",
      "max        32.000000\n",
      "Name: Total_Purchases_Count, dtype: float64\n",
      "\n",
      "Data extraction and selection for channel features complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Selection ---\n",
    "# Combine the core features and the specific channel features selected.\n",
    "features_to_extract = [col for col in CORE_FEATURES + CHANNEL_FEATURES if col in df.columns]\n",
    "\n",
    "# Select only the required columns\n",
    "df_extracted = df[features_to_extract].copy()\n",
    "\n",
    "# --- Initial Assessment of Channel Purchasing Habits ---\n",
    "df_extracted['Total_Purchases_Count'] = (\n",
    "    df_extracted['NumWebPurchases'] + \n",
    "    df_extracted['NumCatalogPurchases'] + \n",
    "    df_extracted['NumStorePurchases']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Total Purchases Count Summary ---\")\n",
    "print(df_extracted['Total_Purchases_Count'].describe())\n",
    "\n",
    "print(\"\\nData extraction and selection for channel features complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbf80a",
   "metadata": {},
   "source": [
    "### 4. Feature Transformation\n",
    "This step implements MinMaxScaler on a selection of numerical features, including Income and the original channel count variables. The purpose of this transformation is to normalize the scale of these predictors to a uniform range between 0 and 1. Normalization is essential because it prevents variables with inherently large raw values, such as Income, from numerically dominating the model's objective function during training. Finally, the script demonstrates re-creating a purchase count feature using the new scaled channel variables for assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac922cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling applied successfully to Income and Channel Features.\n",
      "\n",
      "--- Total Scaled Purchases Count Summary ---\n",
      "count    2240.000000\n",
      "mean        0.691761\n",
      "std         0.380945\n",
      "min         0.000000\n",
      "25%         0.340558\n",
      "50%         0.646825\n",
      "75%         0.987815\n",
      "max         1.651913\n",
      "Name: Total_Purchases_Count_Scaled, dtype: float64\n",
      "\n",
      "Data scaling complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Feature Transformation (Scaling Numerical Data) ---\n",
    "# Define all numerical features that need scaling, including Income\n",
    "SCALING_FEATURES = ['Income'] + CHANNEL_FEATURES\n",
    "\n",
    "# Initialize the Scaler (MinMaxScaler is robust for skewed count/revenue data)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the scaling transformation in place\n",
    "df_extracted[SCALING_FEATURES] = scaler.fit_transform(df_extracted[SCALING_FEATURES])\n",
    "print(\"\\nScaling applied successfully to Income and Channel Features.\")\n",
    "\n",
    "\n",
    "# Example: Re-create the new feature using scaled values\n",
    "df_extracted['Total_Purchases_Count_Scaled'] = (\n",
    "    df_extracted['NumWebPurchases'] + \n",
    "    df_extracted['NumCatalogPurchases'] + \n",
    "    df_extracted['NumStorePurchases']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Total Scaled Purchases Count Summary ---\")\n",
    "print(df_extracted['Total_Purchases_Count_Scaled'].describe())\n",
    "\n",
    "print(\"\\nData scaling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e5b94",
   "metadata": {},
   "source": [
    "### 5. Engineer new useful features.\n",
    "This step focuses on creating highly predictive behavioral and monetary features from the raw data. It calculates Total_Purchases_Count as a simple measure of engagement across channels to assess customer activity. Crucially, it creates Total_Spent, which acts as a robust proxy for customer lifetime value (CLV) and will be a strong model predictor. Additionally, the Prop_Online_Purchases ratio is calculated to capture a customer's channel preference (online vs. physical store). These new features capture deeper insight into customer wealth and behavioral habits than the original columns alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34119de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineered features (Total_Purchases_Count, Prop_Online_Purchases, Total_Spent) created successfully.\n",
      "Scaling applied successfully to Income, Spending, and Channel Features.\n",
      "\n",
      "--- Statistical Summary of Key Engineered Features ---\n",
      "                        count      mean       std  min       25%       50%  \\\n",
      "Total_Spent            2240.0  0.238412  0.238988  0.0  0.025298  0.155159   \n",
      "Total_Purchases_Count  2240.0  0.391783  0.225179  0.0  0.187500  0.375000   \n",
      "Prop_Online_Purchases  2240.0  0.018055  0.005491  0.0  0.014815  0.018330   \n",
      "\n",
      "                            75%       max  \n",
      "Total_Spent            0.412897  1.000000  \n",
      "Total_Purchases_Count  0.562500  1.000000  \n",
      "Prop_Online_Purchases  0.021693  0.037037  \n"
     ]
    }
   ],
   "source": [
    "# Calculate Proportion of Online Purchases (Web + Catalog Purchases / Total Purchases)\n",
    "df_extracted['Prop_Online_Purchases'] = (\n",
    "    df_extracted['NumWebPurchases'] + df_extracted['NumCatalogPurchases']\n",
    ") / df_extracted['Total_Purchases_Count'].replace(0, 1) # Replace 0 with 1 temporarily to avoid zero division\n",
    "df_extracted.loc[df_extracted['Total_Purchases_Count'] == 0, 'Prop_Online_Purchases'] = 0\n",
    "\n",
    "# Calculate Total Spending (Total Monetary Value)\n",
    "df_extracted['Total_Spent'] = (\n",
    "    df_extracted['MntWines'] + df_extracted['MntFruits'] + df_extracted['MntMeatProducts'] + \n",
    "    df_extracted['MntFishProducts'] + df_extracted['MntSweetProducts'] + df_extracted['MntGoldProds']\n",
    ")\n",
    "\n",
    "print(\"\\nEngineered features (Total_Purchases_Count, Prop_Online_Purchases, Total_Spent) created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c94a7f",
   "metadata": {},
   "source": [
    "### 6. Create Dummy Variables\n",
    "This mandatory step converts the two remaining categorical text features (Education and Marital_Status) into a numerical format suitable for machine learning models. Before conversion, the Marital_Status column is cleaned by grouping rare or similar labels (like 'Divorced', 'Widow', and 'Alone') into the unified 'Single' category to reduce noise and simplify the model. The features are then converted using one-hot encoding (pd.get_dummies), and one column is dropped per feature (drop_first=True) to prevent multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b42cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy variables created for 'Education' and 'Marital_Status'.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Dummy Variables (One-Hot Encoding) ---\n",
    "\n",
    "# Clean Marital Status: Combine small/similar groups for simplicity\n",
    "df_extracted['Marital_Status'] = df_extracted['Marital_Status'].replace(['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd'], 'Single')\n",
    "\n",
    "# Create Dummy Variables for Categorical Features\n",
    "df_extracted = pd.get_dummies(df_extracted, columns=['Education', 'Marital_Status'], drop_first=True)\n",
    "print(\"Dummy variables created for 'Education' and 'Marital_Status'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ea864",
   "metadata": {},
   "source": [
    "### 7. Feature Transformation\n",
    "This final preparation step applies MinMaxScaler to the core numerical features, including the newly engineered Total_Spent and Total_Purchases_Count. Normalizing these predictors ensures that features with vastly different scales—like Income and the low-count channel visits—contribute fairly to the model training process. Scaling prevents features with larger magnitudes from dominating the loss function, which is critical for optimization. The process is validated by inspecting the descriptive statistics of the scaled engineered features, confirming they are ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfe132d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling applied successfully to Income, Spending, and Channel Features.\n",
      "\n",
      "--- Statistical Summary of Key Engineered Features ---\n",
      "                        count      mean       std  min       25%       50%  \\\n",
      "Total_Spent            2240.0  0.238412  0.238988  0.0  0.025298  0.155159   \n",
      "Total_Purchases_Count  2240.0  0.391783  0.225179  0.0  0.187500  0.375000   \n",
      "Prop_Online_Purchases  2240.0  0.018055  0.005491  0.0  0.014815  0.018330   \n",
      "\n",
      "                            75%       max  \n",
      "Total_Spent            0.412897  1.000000  \n",
      "Total_Purchases_Count  0.562500  1.000000  \n",
      "Prop_Online_Purchases  0.021693  0.037037  \n"
     ]
    }
   ],
   "source": [
    "# --- Feature Transformation (Scaling Numerical Data) ---\n",
    "\n",
    "# Define all numerical features that need scaling, including new engineered ones\n",
    "SCALING_FEATURES = ['Income'] + CHANNEL_FEATURES + ['Total_Purchases_Count', 'Total_Spent', 'Prop_Online_Purchases']\n",
    "SCALING_NUMERICALS = ['Income', 'Total_Spent'] + CHANNEL_FEATURES + ['Total_Purchases_Count']\n",
    "\n",
    "# Initialize the Scaler (MinMaxScaler is robust for skewed count/revenue data)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply the scaling transformation in place\n",
    "df_extracted[SCALING_NUMERICALS] = scaler.fit_transform(df_extracted[SCALING_NUMERICALS])\n",
    "print(\"Scaling applied successfully to Income, Spending, and Channel Features.\")\n",
    "\n",
    "# --- Inspection of Final Transformed Data ---\n",
    "\n",
    "print(\"\\n--- Statistical Summary of Key Engineered Features ---\")\n",
    "print(df_extracted[['Total_Spent', 'Total_Purchases_Count', 'Prop_Online_Purchases']].describe().transpose())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
